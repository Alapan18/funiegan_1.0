{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-_WakDenGmgW",
    "outputId": "1bc85e9c-9a2d-45b5-ba7c-f55d2cdf429b"
   },
   "outputs": [],
   "source": [
    "%cd C:/Users/alapa/funiegan/funiegan_1.0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pHEYOC7cK4bn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import argparse\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "# pytorch libs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "# local libs\n",
    "from PyTorch.nets.commons import Weights_Normal, VGG19_PercepLoss\n",
    "from PyTorch.nets.SelfAttentionfuniegan import GeneratorFunieGAN, DiscriminatorFunieGAN\n",
    "from PyTorch.utils.data_utils import GetTrainingPairs, GetValImage\n",
    "import math\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from torchvision import models\n",
    "from torchvision.models import vgg19, VGG19_Weights\n",
    "from torch_optimizer import Lookahead,NovoGrad\n",
    "from adabelief_pytorch import AdaBelief\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1bpxUYCnK4ZS"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--cfg_file\", type=str, default=r\"PyTorch/configs/train_euvp.yaml\")\n",
    "parser.add_argument(\"--epoch\", type=int, default=72, help=\"which epoch to start from\")\n",
    "parser.add_argument(\"--num_epochs\", type=int, default=400, help=\"number of epochs of training\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=8, help=\"size of the batches\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0001, help=\"adam: learning rate\")\n",
    "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of 1st order momentum\")\n",
    "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of 2nd order momentum\")\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xjYo-WeMK4Iu"
   },
   "outputs": [],
   "source": [
    "epoch = args.epoch\n",
    "num_epochs = args.num_epochs\n",
    "batch_size =  args.batch_size\n",
    "lr_rate, lr_b1, lr_b2 = args.lr, args.b1, args.b2\n",
    "# load the data config file\n",
    "with open(args.cfg_file) as f:\n",
    "    cfg = yaml.load(f, Loader=yaml.FullLoader)\n",
    "# get info from config file\n",
    "dataset_name = cfg[\"dataset_name\"]\n",
    "dataset_path = cfg[\"dataset_path\"]\n",
    "channels = cfg[\"chans\"]\n",
    "img_width = cfg[\"im_width\"]\n",
    "img_height = cfg[\"im_height\"]\n",
    "val_interval = cfg[\"val_interval\"]\n",
    "ckpt_interval = cfg[\"ckpt_interval\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qjCtxcO1RhVK",
    "outputId": "9c48701a-7239-4272-fe7a-76dbf14e793a"
   },
   "outputs": [],
   "source": [
    "\n",
    "print (dataset_name)\n",
    "print (dataset_path)\n",
    "print (channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ZOZK9M4xK4FJ"
   },
   "outputs": [],
   "source": [
    "## create dir for model and validation data\n",
    "samples_dir = os.path.join(\"samples/FunieGAN/\", dataset_name)\n",
    "checkpoint_dir = os.path.join(\"checkpoints/FunieGAN(NLSelf)/\", dataset_name)\n",
    "os.makedirs(samples_dir, exist_ok=True)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSOcWFz7K4C2"
   },
   "outputs": [],
   "source": [
    "Adv_cGAN = torch.nn.MSELoss()\n",
    "L1_G  = torch.nn.L1Loss() # similarity loss (l1)\n",
    "L_vgg = VGG19_PercepLoss() # content loss (vgg)\n",
    "lambda_1, lambda_con = 7, 3 # 7:3 (as in paper)\n",
    "patch = (1, img_height//16, img_width//16) # 16x16 for 256x256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples, distorted_samples, Tensor):\n",
    "    \"\"\"\n",
    "    Calculates the gradient penalty loss for WGAN-GP\n",
    "    \"\"\"\n",
    "    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
    "    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates, distorted_samples)\n",
    "    fake = Variable(Tensor(real_samples.shape[0], *patch).fill_(1.0), requires_grad=False)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kd8IEfefK4AA"
   },
   "outputs": [],
   "source": [
    "generator = GeneratorFunieGAN()\n",
    "discriminator = DiscriminatorFunieGAN()\n",
    "\n",
    "# see if cuda is available\n",
    "if torch.cuda.is_available():\n",
    "    generator = generator.cuda()\n",
    "    discriminator = discriminator.cuda()\n",
    "    Adv_cGAN.cuda()\n",
    "    L1_G = L1_G.cuda()\n",
    "    L_vgg = L_vgg.cuda()\n",
    "    Tensor = torch.cuda.FloatTensor\n",
    "else:\n",
    "    Tensor = torch.FloatTensor\n",
    "\n",
    "# Initialize weights or load pretrained models\n",
    "if args.epoch == 0:\n",
    "    generator.apply(Weights_Normal)\n",
    "    discriminator.apply(Weights_Normal)\n",
    "else:\n",
    "    generator.load_state_dict(torch.load(\"checkpoints/FunieGAN(NLSelf)/%s/generator_%d.pth\" % (dataset_name, args.epoch)))\n",
    "    discriminator.load_state_dict(torch.load(\"checkpoints/FunieGAN(NLSelf)/%s/discriminator_%d.pth\" % (dataset_name, epoch)))\n",
    "    print (\"Loaded model from epoch %d\" %(epoch))\n",
    "\n",
    "# Optimizers\n",
    "\"\"\"\n",
    "base_optim_G = torch.optim.SGD(generator.parameters(), lr=3*lr_rate, momentum=0.9)\n",
    "optimizer_G = Lookahead(base_optim_G, k=5, alpha=0.5)\n",
    "optimizer_D = torch.optim.RAdam(discriminator.parameters(), lr=lr_rate, betas=(lr_b1, lr_b2))\n",
    "scheduler_G = torch.optim.lr_scheduler.StepLR(optimizer_G, step_size=15, gamma=0.85)\n",
    "scheduler_D = torch.optim.lr_scheduler.StepLR(optimizer_D, step_size=20, gamma=0.8)\n",
    "\"\"\"\n",
    "base_optim_G = AdaBelief(generator.parameters(), lr=3*lr_rate, betas=(lr_b1, lr_b2), eps=1e-16, weight_decay=1e-4, rectify=True)\n",
    "optimizer_G = Lookahead(base_optim_G, k=5, alpha=0.5)\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.5*lr_rate, betas=(lr_b1, lr_b2))\n",
    "scheduler_G = torch.optim.lr_scheduler.StepLR(optimizer_G, step_size=25, gamma=0.85)\n",
    "scheduler_D = torch.optim.lr_scheduler.StepLR(optimizer_D, step_size=25, gamma=0.8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kceQwejiK39s",
    "outputId": "2f862cc0-a926-4b5b-9393-8383ad7cbb8d"
   },
   "outputs": [],
   "source": [
    "transforms_ = [\n",
    "    transforms.Resize((img_height, img_width), Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    GetTrainingPairs(dataset_path, dataset_name, transforms_=transforms_),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = 8,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    GetValImage(dataset_path, dataset_name, transforms_=transforms_, sub_dir='validation'),\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DnXdrPRLRhVM",
    "outputId": "b87b4e9e-91b8-4555-f16b-642c8e0a7720"
   },
   "outputs": [],
   "source": [
    "print(f\"Number of samples in dataloader: {len(dataloader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:  # MSE is zero means no noise\n",
    "        return 100\n",
    "    PIXEL_MAX = 1.0  # Images are normalized between 0-1\n",
    "    return 20 * math.log10(PIXEL_MAX / math.sqrt(mse))\n",
    "\n",
    "def validate_metrics(generator, val_dataloader, Tensor):\n",
    "    generator.eval()\n",
    "    psnr_values, ssim_values = [], []\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        imgs_val = Variable(batch[\"val\"].type(Tensor))\n",
    "        imgs_gt = Variable(batch[\"gt\"].type(Tensor))\n",
    "        imgs_gen = generator(imgs_val).detach().cpu().numpy()\n",
    "        imgs_gt = imgs_gt.cpu().numpy()\n",
    "        for gt, gen in zip(imgs_gt, imgs_gen):\n",
    "            psnr_values.append(calculate_psnr(gt, gen))\n",
    "            ssim_values.append(ssim(gt.transpose(1, 2, 0), gen.transpose(1, 2, 0), multichannel=True))\n",
    "    generator.train()\n",
    "    return np.mean(psnr_values), np.mean(ssim_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the VGG Feature Extractor\n",
    "class VGGFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGFeatureExtractor, self).__init__()\n",
    "        #vgg19 = models.vgg19(pretrained=True)\n",
    "        vgg19 = models.vgg19(weights=VGG19_Weights.IMAGENET1K_V1)\n",
    "        self.feature_extractor = nn.Sequential(*list(vgg19.features.children())[:16])  # Use up to conv4_1\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False  # Freeze VGG parameters\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.feature_extractor(img)\n",
    "\n",
    "# Initialize the VGG Feature Extractor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg_extractor = VGGFeatureExtractor().to(device)\n",
    "\n",
    "# VGG Method\n",
    "def VGG_loss(imgs_fake, imgs_good_gt):\n",
    "    \"\"\"\n",
    "    Calculates the VGG-based perceptual loss.\n",
    "    \"\"\"\n",
    "    features_fake = vgg_extractor(imgs_fake)\n",
    "    features_real = vgg_extractor(imgs_good_gt)\n",
    "    loss_content = F.mse_loss(features_fake, features_real)\n",
    "    return loss_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pixel Loss Method\n",
    "def pixel_loss(imgs_fake, imgs_good_gt):\n",
    "    \"\"\"\n",
    "    Calculates the L1 pixel-wise loss between the fake and ground truth images.\n",
    "    \"\"\"\n",
    "    loss_pixel = F.l1_loss(imgs_fake, imgs_good_gt)\n",
    "    return loss_pixel\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vN7EMzpCK37D",
    "outputId": "614a7e74-c490-4a9e-b240-3e3fce86da27"
   },
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(epoch+1, num_epochs):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # Model inputs\n",
    "        imgs_distorted = Variable(batch[\"A\"].type(Tensor))\n",
    "        imgs_good_gt = Variable(batch[\"B\"].type(Tensor))\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(np.ones((imgs_distorted.size(0), *patch))), requires_grad=False)\n",
    "        \n",
    "        fake = Variable(Tensor(np.zeros((imgs_distorted.size(0), *patch))), requires_grad=False)\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        imgs_fake = generator(imgs_distorted).detach()  # Ensure no gradients for the generator\n",
    "        pred_real = discriminator(imgs_good_gt, imgs_distorted)\n",
    "        loss_real = Adv_cGAN(pred_real, valid)\n",
    "        pred_fake = discriminator(imgs_fake, imgs_distorted)\n",
    "        loss_fake = Adv_cGAN(pred_fake, fake)\n",
    "        # Gradient Penalty\n",
    "        gradient_penalty = compute_gradient_penalty(discriminator, imgs_good_gt, imgs_fake, imgs_distorted, Tensor)\n",
    "        # Total Discriminator Loss\n",
    "        loss_D = 0.5 * (loss_real + loss_fake) + 5.0 * gradient_penalty\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train Generator\n",
    "        #for _ in range(2):\n",
    "        optimizer_G.zero_grad()\n",
    "        imgs_fake = generator(imgs_distorted)\n",
    "        pred_fake = discriminator(imgs_fake, imgs_distorted)\n",
    "        loss_GAN = Adv_cGAN(pred_fake, valid)\n",
    "        loss_pixel = pixel_loss(imgs_fake, imgs_good_gt)\n",
    "        loss_content = VGG_loss(imgs_fake, imgs_good_gt)\n",
    "        loss_G = 0.1 * loss_GAN + 8.0 * loss_pixel + 0.5 * loss_content\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        ## Print log\n",
    "        if not i%1:\n",
    "            sys.stdout.write(\"\\r[Epoch %d/%d: batch %d/%d] [DLoss: %.3f, GLoss: %.3f, AdvLoss: %.3f]\"\n",
    "                              %(\n",
    "                                epoch, num_epochs, i, len(dataloader),\n",
    "                                loss_D.item(), loss_G.item(), loss_GAN.item(),\n",
    "                               )\n",
    "            )\n",
    "        ## If at sample interval save image\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % val_interval == 0:\n",
    "            imgs = next(iter(val_dataloader))\n",
    "            imgs_val = Variable(imgs[\"val\"].type(Tensor))\n",
    "            imgs_gen = generator(imgs_val)\n",
    "            img_sample = torch.cat((imgs_val.data, imgs_gen.data), -2)\n",
    "            save_image(img_sample, \"samples/FunieGAN/%s/%s.png\" % (dataset_name, batches_done), nrow=5, normalize=True)\n",
    "    scheduler_G.step()\n",
    "    scheduler_D.step()\n",
    "    if batches_done % val_interval == 0:\n",
    "        psnr, ssim_val = validate_metrics(generator, val_dataloader, Tensor)\n",
    "        print(f\"Validation Metrics - PSNR: {psnr:.2f}, SSIM: {ssim_val:.2f}\")\n",
    "\n",
    "    ## Save model checkpoints\n",
    "    torch.save(generator.state_dict(), \"checkpoints/FunieGAN(NLSelf)/%s/generator_%d.pth\" % (dataset_name, epoch))\n",
    "    torch.save(discriminator.state_dict(), \"checkpoints/FunieGAN(NLSelf)/%s/discriminator_%d.pth\" % (dataset_name, epoch))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

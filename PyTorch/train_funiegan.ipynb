{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_WakDenGmgW",
        "outputId": "ff3cd0b3-7f88-416d-be5b-2268b66c3cd5"
      },
      "outputs": [],
      "source": [
        "%cd C:/Users/alapa/funiegan/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pHEYOC7cK4bn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import yaml\n",
        "import argparse\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "# pytorch libs\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "import torchvision.transforms as transforms\n",
        "# local libs\n",
        "from PyTorch.nets.commons import Weights_Normal, VGG19_PercepLoss\n",
        "from PyTorch.nets.funiegan import GeneratorFunieGAN, DiscriminatorFunieGAN\n",
        "from PyTorch.utils.data_utils import GetTrainingPairs, GetValImage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1bpxUYCnK4ZS"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--cfg_file\", type=str, default=r\"PyTorch/configs/train_euvp.yaml\")\n",
        "parser.add_argument(\"--epoch\", type=int, default=0, help=\"which epoch to start from\")\n",
        "parser.add_argument(\"--num_epochs\", type=int, default=201, help=\"number of epochs of training\")\n",
        "parser.add_argument(\"--batch_size\", type=int, default=8, help=\"size of the batches\")\n",
        "parser.add_argument(\"--lr\", type=float, default=0.0003, help=\"adam: learning rate\")\n",
        "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of 1st order momentum\")\n",
        "parser.add_argument(\"--b2\", type=float, default=0.99, help=\"adam: decay of 2nd order momentum\")\n",
        "args, unknown = parser.parse_known_args()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xjYo-WeMK4Iu"
      },
      "outputs": [],
      "source": [
        "epoch = args.epoch\n",
        "num_epochs = args.num_epochs\n",
        "batch_size =  args.batch_size\n",
        "lr_rate, lr_b1, lr_b2 = args.lr, args.b1, args.b2\n",
        "# load the data config file\n",
        "with open(args.cfg_file) as f:\n",
        "    cfg = yaml.load(f, Loader=yaml.FullLoader)\n",
        "# get info from config file\n",
        "dataset_name = cfg[\"dataset_name\"]\n",
        "dataset_path = cfg[\"dataset_path\"]\n",
        "channels = cfg[\"chans\"]\n",
        "img_width = cfg[\"im_width\"]\n",
        "img_height = cfg[\"im_height\"]\n",
        "val_interval = cfg[\"val_interval\"]\n",
        "ckpt_interval = cfg[\"ckpt_interval\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print (dataset_name)\n",
        "print (dataset_path)\n",
        "print (channels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZOZK9M4xK4FJ"
      },
      "outputs": [],
      "source": [
        "## create dir for model and validation data\n",
        "samples_dir = os.path.join(\"samples/FunieGAN/\", dataset_name)\n",
        "checkpoint_dir = os.path.join(\"checkpoints/FunieGAN/\", dataset_name)\n",
        "os.makedirs(samples_dir, exist_ok=True)\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSOcWFz7K4C2",
        "outputId": "247223fa-19cf-4dd8-e24c-85817f44299f"
      },
      "outputs": [],
      "source": [
        "Adv_cGAN = torch.nn.MSELoss()\n",
        "L1_G  = torch.nn.L1Loss() # similarity loss (l1)\n",
        "L_vgg = VGG19_PercepLoss() # content loss (vgg)\n",
        "lambda_1, lambda_con = 7, 3 # 7:3 (as in paper)\n",
        "patch = (1, img_height//16, img_width//16) # 16x16 for 256x256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Kd8IEfefK4AA"
      },
      "outputs": [],
      "source": [
        "generator = GeneratorFunieGAN()\n",
        "discriminator = DiscriminatorFunieGAN()\n",
        "\n",
        "# see if cuda is available\n",
        "if torch.cuda.is_available():\n",
        "    generator = generator.cuda()\n",
        "    discriminator = discriminator.cuda()\n",
        "    Adv_cGAN.cuda()\n",
        "    L1_G = L1_G.cuda()\n",
        "    L_vgg = L_vgg.cuda()\n",
        "    Tensor = torch.cuda.FloatTensor\n",
        "else:\n",
        "    Tensor = torch.FloatTensor\n",
        "\n",
        "# Initialize weights or load pretrained models\n",
        "if args.epoch == 0:\n",
        "    generator.apply(Weights_Normal)\n",
        "    discriminator.apply(Weights_Normal)\n",
        "else:\n",
        "    generator.load_state_dict(torch.load(\"checkpoints/FunieGAN/%s/generator_%d.pth\" % (dataset_name, args.epoch)))\n",
        "    discriminator.load_state_dict(torch.load(\"checkpoints/FunieGAN/%s/discriminator_%d.pth\" % (dataset_name, epoch)))\n",
        "    print (\"Loaded model from epoch %d\" %(epoch))\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr_rate, betas=(lr_b1, lr_b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr_rate, betas=(lr_b1, lr_b2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kceQwejiK39s",
        "outputId": "ce699cf4-da92-424f-f884-ce9199ccbb22"
      },
      "outputs": [],
      "source": [
        "transforms_ = [\n",
        "    transforms.Resize((img_height, img_width), Image.BICUBIC),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "]\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    GetTrainingPairs(dataset_path, dataset_name, transforms_=transforms_),\n",
        "    batch_size = batch_size,\n",
        "    shuffle = True,\n",
        "    num_workers = 8,\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    GetValImage(dataset_path, dataset_name, transforms_=transforms_, sub_dir='validation'),\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Number of samples in dataloader: {len(dataloader.dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN7EMzpCK37D",
        "outputId": "a938ad82-2b19-443a-e41b-637707fc8156"
      },
      "outputs": [],
      "source": [
        "for epoch in range(epoch, num_epochs):\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        # Model inputs\n",
        "        imgs_distorted = Variable(batch[\"A\"].type(Tensor))\n",
        "        imgs_good_gt = Variable(batch[\"B\"].type(Tensor))\n",
        "        # Adversarial ground truths\n",
        "        valid = Variable(Tensor(np.ones((imgs_distorted.size(0), *patch))), requires_grad=False)\n",
        "        fake = Variable(Tensor(np.zeros((imgs_distorted.size(0), *patch))), requires_grad=False)\n",
        "\n",
        "        ## Train Discriminator\n",
        "        optimizer_D.zero_grad()\n",
        "        imgs_fake = generator(imgs_distorted)\n",
        "        pred_real = discriminator(imgs_good_gt, imgs_distorted)\n",
        "        loss_real = Adv_cGAN(pred_real, valid)\n",
        "        pred_fake = discriminator(imgs_fake, imgs_distorted)\n",
        "        loss_fake = Adv_cGAN(pred_fake, fake)\n",
        "        # Total loss: real + fake (standard PatchGAN)\n",
        "        loss_D = 0.5 * (loss_real + loss_fake) * 10.0 # 10x scaled for stability\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        ## Train Generator\n",
        "        optimizer_G.zero_grad()\n",
        "        imgs_fake = generator(imgs_distorted)\n",
        "        pred_fake = discriminator(imgs_fake, imgs_distorted)\n",
        "        loss_GAN =  Adv_cGAN(pred_fake, valid) # GAN loss\n",
        "        loss_1 = L1_G(imgs_fake, imgs_good_gt) # similarity loss\n",
        "        loss_con = L_vgg(imgs_fake, imgs_good_gt)# content loss\n",
        "        # Total loss (Section 3.2.1 in the paper)\n",
        "        loss_G = loss_GAN + lambda_1 * loss_1  + lambda_con * loss_con\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        ## Print log\n",
        "        if not i%50:\n",
        "            sys.stdout.write(\"\\r[Epoch %d/%d: batch %d/%d] [DLoss: %.3f, GLoss: %.3f, AdvLoss: %.3f]\"\n",
        "                              %(\n",
        "                                epoch, num_epochs, i, len(dataloader),\n",
        "                                loss_D.item(), loss_G.item(), loss_GAN.item(),\n",
        "                               )\n",
        "            )\n",
        "        ## If at sample interval save image\n",
        "        batches_done = epoch * len(dataloader) + i\n",
        "        if batches_done % val_interval == 0:\n",
        "            imgs = next(iter(val_dataloader))\n",
        "            imgs_val = Variable(imgs[\"val\"].type(Tensor))\n",
        "            imgs_gen = generator(imgs_val)\n",
        "            img_sample = torch.cat((imgs_val.data, imgs_gen.data), -2)\n",
        "            save_image(img_sample, \"samples/FunieGAN/%s/%s.png\" % (dataset_name, batches_done), nrow=5, normalize=True)\n",
        "\n",
        "    ## Save model checkpoints\n",
        "    if (epoch % ckpt_interval == 0):\n",
        "        torch.save(generator.state_dict(), \"checkpoints/FunieGAN/%s/generator_%d.pth\" % (dataset_name, epoch))\n",
        "        torch.save(discriminator.state_dict(), \"checkpoints/FunieGAN/%s/discriminator_%d.pth\" % (dataset_name, epoch))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
